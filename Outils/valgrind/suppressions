###
###  HYPRE
###

# Triggered by 
#   VAHL_DAVIS_VEFPreP1B_impl
#   PETSC_Quasi_Comp_Coupl_keps_VEF_impl_GMRES
#   ThHyd_keps_VEF_impl
#   PETSC_Canal_VEF_2D_LongMelangeThHyd_keps_VEF_impl 
#   Couplage_Implicite_Instationnaire_jdd1
#   PETSC_GCP_SPAI_P0P1Pa
#
{
   <hypre_set_up_factor>
   Memcheck:Cond
   fun:MPIC_Waitall
   fun:MPIR_Alltoall_intra
   fun:MPIR_Alltoall
   fun:MPIR_Alltoall_impl
   fun:PMPI_Alltoall
   ...
   fun:PCSetUp_HYPRE
   fun:PCSetUp
   fun:KSPSetUp
   ...
}
# MPCube - cas 'standard_hydraulic_head_diffusion_0013'
{
   <hypre_setup_6>
   Memcheck:User
   fun:check_mem_is_defined_untyped
   fun:walk_type_array
   fun:check_mem_is_defined
   fun:generic_Send
   fun:PMPI_Send
   fun:hypre_DataExchangeList
   fun:hypre_NewCommPkgCreate_core
   fun:hypre_MatvecCommPkgCreate
   fun:hypre_BoomerAMGCreateS
   fun:hypre_BoomerAMGSetup
   fun:PCSetUp_HYPRE
   fun:PCSetUp
   fun:KSPSetUp
   ...
}
## 06/11/2018: TMA TRUST for update of Petsc in version 3.10
## on is234639 is221556 Ub16.04, g++ 5.4.0, mpich 3.2
## for tests cases:
## Couplage_Implicite_Instationnaire_jdd1
## ThHyd_keps_VEF_impl
## VAHL_DAVIS_VEFPreP1B_impl
{
   <update_to_petsc_3_10_a>
   Memcheck:Leak
   fun:malloc
   fun:hypre_MAlloc
   fun:hypre_idx_malloc
   fun:hypre_SetUpFactor
   fun:hypre_SetUpLUFactor
   fun:HYPRE_DistributedMatrixPilutSolverSetup
   fun:HYPRE_ParCSRPilutSetup
}
{
   <update_to_petsc_3_10_b>
   Memcheck:Leak
   fun:malloc
   fun:hypre_MAlloc
   fun:hypre_mymalloc
   fun:hypre_SetUpFactor
   fun:hypre_SetUpLUFactor
   fun:HYPRE_DistributedMatrixPilutSolverSetup
   fun:HYPRE_ParCSRPilutSetup
}
{
   <update_to_petsc_3_10_c>
   Memcheck:Leak
   fun:calloc
   fun:hypre_CAlloc
   fun:hypre_DistributedMatrixCreate
   fun:HYPRE_DistributedMatrixCreate
   fun:HYPRE_ConvertParCSRMatrixToDistributedMatrix
   fun:HYPRE_ParCSRPilutSetup
}
{
   <update_to_petsc_3_10_d>
   Memcheck:Leak
   fun:malloc
   fun:hypre_MAlloc
   fun:hypre_fp_malloc_init
   fun:hypre_SetUpFactor
   fun:hypre_SetUpLUFactor
   fun:HYPRE_DistributedMatrixPilutSolverSetup
   fun:HYPRE_ParCSRPilutSetup
}
###
###  MUMPS
###

# Triggered by 3D_P0P1Pa_perio
# Triggered by OpDiffP1B_mixte_vitesse_implicite_3D
# Triggered by ModeleCoeur tests NR with Petsc solver (ex: FETUNA_91_CM_T11_jdd5)
{
   <MUMPS_para0>
   Memcheck:Cond
   fun:dmumps_solve_driver_
   fun:dmumps_
   fun:dmumps_f77_
   fun:dmumps_c
   fun:MatSolve_MUMPS
   ...
}
# Triggered by PAR_3D_P0P1Pa for example:
# Error type: User: Uninitialised byte(s) found during client check request
# Triggered by "Execute_parallel" or CasCouplage_jdd2
{
   <MUMPS_para2>
   Memcheck:User
   fun:check_mem_is_defined_untyped
   ...
   fun:dmumps_solve_driver_
   fun:dmumps_
   fun:dmumps_f77_
   fun:dmumps_c
   fun:MatSolve_MUMPS
   ...
}



# Triggered by 
#   PAR_Collecteur
#   PAR_P1toP0P1Pa, 
#   PAR_Test_solveur_sym
#   PAR_DarcyFlow_jdd3
#   PAR_DarcyFlow_jdd4
# Triggered by TrioCFD tests NR launched in parallel with Petsc solver
#   PAR_cpu_3D_VEF
#   PAR_Fiche_validation_Re395_Pr0.71_jdd1
#   les_Re395Pr0025_T0Q_jdd5
# Triggered by CHCa2 on PXP
# Triggered by CasCouplage_jdd[2-4] ICoCo tests NR on is210376, g++ 7.1.1, Fedora 26
{
   <MUMPS_para9>
   Memcheck:Leak
   fun:*alloc
   ...
   fun:dmumps_
   fun:dmumps_f77_
   ...
}

###
### Other PETSC (from PXP mainly)
###
{
   <pxp_LU_2>
   Memcheck:Leak
   match-leak-kinds: definite
   fun:*alloc
   ...
   fun:ompi_comm_activate
   fun:ompi_comm_dup_with_info
   fun:PMPI_Comm_dup
   fun:PetscInitializeMUMPS
   fun:MatGetFactor_aij_mumps
   fun:MatGetFactor
   fun:PCSetUp_LU
   fun:PCSetUp
}
{
   <pxp_5>
   Memcheck:Leak
   fun:*alloc
   ...
   fun:ompi_comm_dup_with_info
   fun:PMPI_Comm_dup
   fun:PetscCommDuplicate
   fun:PetscHeaderCreate_Private
}
{
   <pxp_3>
   Memcheck:Leak
   match-leak-kinds: definite
   fun:*alloc
   ...
   fun:PetscViewerASCIIOpen
   fun:PetscFinalize
}

###
### Intel related suppressions
###

# Upwind on ceres2
{
   <intel_strcpy>
   Memcheck:Cond
   fun:__intel_sse2_strcpy
   ...
}
{
   <intel_strchr>
   Memcheck:Cond
   fun:__intel_sse2_strchr
   ...
}
{
   <intel_strncmp>
   Memcheck:Addr16
   fun:__intel_sse2_strncmp
   ...
}
{
   <intel_strncmp2>
   Memcheck:Cond
   fun:__intel_sse2_strncmp
   ...
}
{
   <split_mot_intel>
   Memcheck:Cond
   fun:_Z9split_motRK3Nom
   ...
}


###
###  CLANG related suppressions
###

# upwind on clang++ 3.8.0, Fedora 24 (is210376)
# upwind and PAR_upwind tests on is210376, clang++ 4.0.0, Fedora 26
# Triggered by CHCa2 on PXP
{
   <upwind_clang_3_8_0_0>
   Memcheck:Leak
   fun:*alloc
   ...
   fun:opal_libevent2022_event_base_loop
   ...
   fun:start_thread
   fun:clone
}
{
   <upwind_clang_3_8_0_1>
   Memcheck:Cond
   fun:opal_value_unload
   fun:rte_init
   fun:orte_init
   fun:ompi_mpi_init
   fun:PMPI_Init_thread
   fun:PetscInitialize
   ...
}
{
   <upwind_clang_3_8_0_6>
   Memcheck:Leak
   match-leak-kinds: reachable
   fun:*alloc
   ...
   fun:ompi_mpi_init
   fun:PMPI_Init_thread
   fun:PetscInitialize
   ...
}
{
   <upwind_clang_3_8_0_9>
   Memcheck:Leak
   fun:*alloc
   fun:opal_class_initialize
   ...
   fun:PMPI_Init_thread
   fun:PetscInitialize
   ...
}
# Also triggered 
{ 
   <upwind_clang_3_8_0_42>
   Memcheck:Leak
   fun:malloc
   ...
   fun:orte_finalize
   fun:ompi_mpi_finalize
   fun:PetscFinalize
   ...
}
{
   <upwind_clang_4_0_0_3>
   Memcheck:Leak
   match-leak-kinds: reachable
   fun:malloc
   ...
   fun:PMPI_Bcast
   fun:PetscOptionsInsertFile
   fun:PetscOptionsInsert
   fun:PetscInitialize
   ...
}


###
### OpenMPI related suppressions
###

# upwind - is223279 en Fedora 18 - g++ 4.7.2 - OpenMPI 2.0.2
# is147462 en Fedora 22 - g++ 5.1.1 - OpenMPI 2.0.2
# Triggered by upwind and PAR_upwind tests with OpenMPI 2.0.2 (provided library)
# on is210968, g++ 4.8.5, CentOS 7.3

# The below is quite large, but on Clang for example we see tons of errors like the one below! Even string functions looks leaky ...
#{
#   <insert_a_suppression_name_here>
#   Memcheck:Leak
#   match-leak-kinds: definite
#   fun:realloc
#   fun:vasprintf
#   fun:asprintf
#   fun:ompi_mpi_init
#   fun:PMPI_Init_thread
#   fun:PetscInitialize
#}

{
   <upwind_openmpi_2_0_2_4>
   Memcheck:Leak
   fun:*alloc
   ...
   fun:ompi_mpi_init
   fun:PMPI_Init_thread
   fun:PetscInitialize*
   ...
}
# From Adela PUSCAS:
{
   <upwind_openmpi_2_0_2_5>
   Memcheck:Leak
   fun:memalign
   ...
   fun:hwloc_topology_load
   fun:opal_hwloc_base_get_topology
   fun:ompi_mpi_init
   fun:PMPI_Init_thread
   fun:PetscInitialize*
   ...
}

# upwind -- is225222 en Fedora 18 - g++ 4.7.2 - OpenMPI 2.1.1
{
   <ompi_2_1_1__1>
   Memcheck:Cond
   fun:opal_value_unload
   ...
   fun:ompi_mpi_init
   fun:PMPI_Init
   ...
}
{
   <ompi_2_1_1__2>
   Memcheck:Leak
   match-leak-kinds: reachable
   fun:malloc
   ...
   fun:opal_argv_append_nosize
   fun:opal_argv_append
   fun:opal_argv_split_inter
   fun:dlopen_component_register
   fun:mca_base_framework_components_register
   fun:mca_base_framework_register
   fun:mca_base_framework_open
   fun:mca_base_component_repository_init
   fun:mca_base_open
   fun:opal_init_util
   fun:ompi_mpi_init
   ...
}

{
   <upwind_openmpi_2_0_2_5>
   Memcheck:Leak
   match-leak-kinds: reachable
   fun:malloc
   fun:opal_class_initialize
   fun:opal_obj_new.constprop.9
   fun:mca_pml_ob1_isend
   fun:ompi_coll_base_bcast_intra_generic
   fun:ompi_coll_base_bcast_intra_binomial
   fun:ompi_coll_tuned_bcast_intra_dec_fixed
   fun:PMPI_Bcast
   fun:PetscOptionsInsertFile
   fun:PetscOptionsInsert
   fun:PetscInitialize.part.3
   ...
   fun:main
}
## on is223280, g++ 4.7.2, Fedora 18
{
   <upwind_openmpi_2_0_2_8>
   Memcheck:Leak
   match-leak-kinds: reachable
   fun:malloc
   fun:opal_class_initialize
   ...
   fun:ompi_coll_base_bcast_intra_generic
   fun:ompi_coll_base_bcast_intra_binomial
   fun:ompi_coll_tuned_bcast_intra_dec_fixed
   fun:PMPI_Bcast
   fun:PetscOptionsInsertFile
   fun:PetscOptionsInsert
   fun:PetscInitialize.part.3
   ...
   fun:main
}
## on is227246, g++ 7.1.1, Fedora 26
{
   <upwind_openmpi_2_0_2_10>
   Memcheck:Leak
   ...
   fun:PMPI_Bcast
   fun:PetscOptionsInsertFile
   fun:PetscOptionsInsert
   fun:PetscInitialize.part.2
   ...
}
{
   <upwind_openmpi_2_0_2_11>
   Memcheck:Leak
   fun:*alloc
   ...
   fun:opal_libevent2022_event_add
   ...
   fun:opal_init
   fun:orte_init
   fun:ompi_mpi_init
}
# upwind on is212798 (Fed18)
{
   <upwind_openmpi_2_0_2_12>
   Memcheck:Cond
   fun:opal_value_unload
   fun:ompi_proc_complete_init
   fun:ompi_mpi_init
   fun:PMPI_Init_thread
   fun:PetscInitialize*
   ...  
}

###
###  Various (and ugly ...)
###

# On some hosts/OS dl_init is leaking!!!
# Typically happens with libstdc++.so.6.XXX (e.g. on FED22 with g++5.1.1), 'upwind' case
{
   <dl_init>
   Memcheck:Leak
   match-leak-kinds: reachable
   fun:malloc
   obj:/usr/lib64/libstdc++.so.6.*
   fun:call_init.part.*
   fun:_dl_init
   obj:/usr/lib64/ld-*.so
   ...
}
# Similar as above - Triggered by upwind 
#    is229106 en Mint 18.1 Serena - g++ 5.4.0 - MPICH 3.2
#    is226426 en Ubuntu 16.04 - g++ 5.4.0 - MPICH 3.2
{
   <dl_init2>
   Memcheck:Leak
   match-leak-kinds: reachable
   fun:malloc
   obj:/usr/lib/x86_64-linux-gnu/libstdc++.so.6.*
   fun:call_init.part.0
   fun:call_init
   fun:_dl_init
   obj:/lib/x86_64-linux-gnu/ld-2.*.so
   ...
}
# Triggered by upwind -- is223196 en CentOS 6.4 - g++ 6.3.0 - MPICH 3.2
{
   <free_gpp630>
   Memcheck:Free
   fun:free
   fun:free_mem
   fun:__libc_freeres
   fun:_vgnU_freeres
   fun:exit
   ...
}
#  dlopen_jdd1: more complex case: inside this case, dynamic.data is ran first (in the prepare I beleive)
#  This is the first log to monitor before looking at dlopen_jdd1.err ...
{
   <dlopen_jdd1>
   Memcheck:Leak
   match-leak-kinds: reachable
   fun:calloc
   fun:_dlerror_run
   fun:dlopen@@GLIBC_2.2.5
   ...
}
# Triggered by CHCa2 on PXP
{
   <pxp_openmpi2>
   Memcheck:Leak
   ...
   fun:*alloc
   ...
   fun:dlopen@@GLIBC_2.2.5
   obj:/usr/lib*/openmpi/lib/libopen-pal.so.*
   ...
}


##
##  Callisto with OpenMPI
##
{
   <callisto_openmpi_1>
   Memcheck:Cond
   ...
   fun:ompi_mtl_psm_module_init
   fun:ompi_mtl_psm_component_init
   fun:ompi_mtl_base_select
   fun:mca_pml_cm_component_init
   fun:mca_pml_base_select
   fun:ompi_mpi_init
   fun:PMPI_Init_thread
}

{
   <callisto_openmpi_2>
   Memcheck:Leak   
   ...
   fun:opal_db_base_commit
   fun:modex
   fun:ompi_mpi_init
   fun:PMPI_Init_thread
   fun:PetscInitialize
}
{
   <callisto_openmpi_3>
   Memcheck:Leak
   ...
   fun:ipath_poll_type
   fun:psmi_context_interrupt_set
   fun:ips_ptl_rcvthread_fini
   fun:ips_ptl_fini
   fun:psm_ep_close
   fun:ompi_mtl_psm_finalize
   fun:ompi_mpi_finalize
   fun:PetscFinalize
}
{
   <callisto_openmpi_4>
   Memcheck:Leak
   match-leak-kinds: reachable
   fun:malloc
   fun:strdup
   fun:init_ipath_mylabel
   obj:/usr/lib64/libinfinipath.so.*
   ...
}
{
   <callisto_openmpi_5>
   Memcheck:Leak
   fun:*alloc
   ...
   fun:ompi_mtl_psm_module_init
   fun:ompi_mtl_psm_component_init
   fun:ompi_mtl_base_select
   fun:mca_pml_cm_component_init
   fun:mca_pml_base_select
}

##
## 06/12/2017: Part added by TMA but to be validated by TRUST project leader
## Triggered by upwind and PAR_upwind tests with OpenMPI 2.0.4 (provided library)
## On these suppressions, we have not verified that the leaks are not incremental.
##
## on is223280, g++ 4.7.2, Fedora 18
{
   <upwind_openmpi_2_0_4_0>
   Memcheck:Leak
   match-leak-kinds: indirect
   fun:malloc
   fun:opal_pmix_pmix112_pmix_bfrop_unpack_string
   fun:opal_pmix_pmix112_pmix_bfrop_unpack_buffer
   fun:unpack_val
   fun:opal_pmix_pmix112_pmix_bfrop_unpack_kval
   fun:opal_pmix_pmix112_pmix_bfrop_unpack_buffer
   fun:opal_pmix_pmix112_pmix_bfrop_unpack
   fun:opal_pmix_pmix112_pmix_client_process_nspace_blob
   fun:job_data
   fun:opal_pmix_pmix112_pmix_usock_process_msg
   fun:event_process_active_single_queue
   fun:event_process_active
   fun:opal_libevent2022_event_base_loop
   fun:progress_engine
   fun:start_thread
}

